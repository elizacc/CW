Метод SHAP несколько отличается от использования значений Шэпли напрямую для интерпретации вклада признаков -- он модифицирует классический подход. Рассмотрим SHAP подробнее.

Вместо использования большого количества моделей мы обучим одну модель и далее будем пользоваться только ее предсказаниями. Тогда наша модель представима в виде $f: \re^d \rightarrow \re$. Вместо исключения из нее признаков (в таком случае нам пришлось бы переобучать модель) мы оставим данные признаки в виде случайных величин и посчитаем математическое ожидание предсказания при фиксированных включенных в модель признаках.

Для упрощения расчетов мы не будем рассматривать разные значения предикторов, а бинаризуем их представление, обозначив за <<1>> наличие предиктора и за <<0>> его отсутствие: $x \rightarrow x', \, x' = \{1\}^d$. % не знаю корректно ли
Обозначим за $z' \in \{0,1\}^d$ объект, у которого мы учитываем только признаки из $S$ при формировании предсказания:
$z_j =
\begin{cases}
1, \text{если $j \in S$}\\
0, \text{если $j \notin S$}\\
\end{cases}$.\\
То есть $z'$ -- одна из возможных комбинаций предикторов.

Обученная модель работает с исходным видом признаков, поэтому нам нужно также восстанавливать значения исходных признаков по бинарному вектору. Введем для этого функцию $h_x(z') = z$, где $x$ -- исходный вектор исследуемого объекта, $z'$ -- бинарный вектор, в котором некоторые признаки заменены нулями, $z$ -- представление вектора $z'$ в исходном пространстве признаков. Функция $h_x$ вместо единиц восстанавливает значения из вектора $x$, а вместо нулей оставляет признак как некоторую переменную (случайную величину).
% наверняка можно как-то умнее упрощать признаки
Тогда наше ожидаемое предсказание для $z$ представимо в виде математического ожидания:
\[
\bar{f}(z) = \e(f(z)|\,z_S)
\]

Мы знаем конкретные значения признаков $z_S$, так как функция $h_x$ перенесла их из объекта $x$. Найдя математическое ожидание мы можем подставить данные значения, чтобы получить условное математическое ожидание, которое и будет оценкой нашего предсказания. И уже данную формулу мы можем использовать при расчете значений Шэпли.% здесь для его расчета мы почему-то переходили к безусловному матожиданию, это связано со свойствами значений Шэпли

Но данные расчеты все еще довольно-таки трудоемкие, так как нам нужно перебрать все возможные комбинации признаков. Даже при обучении одной модели это затратно. Поэтому чтобы еще сильнее упростить расчеты мы воспользуемся свойствами, которыми обладают значения Шэпли. % если понадобится, то лучше все-таки вывести данные свойства отдельно. в целом алгоритм понятен, они тоже задают некоторые веса, тоже обучают модель, и тоже получают веса как значения

Мысли, которые нужно включить сюда:

+1. Как считается value

+2. Есть value, есть вклад, есть прогноз -- надо понять как вклад связан с value

+3. Есть интерпретация -- можно вывести из предпосылок формулу value

4. Я не хочу выводить этот алгоритм через аддитивные модели -- я хочу попроще написать, а потом указать что она аддитивная и показать, что это

5. По возможности расписать как их находить и что с ними потом делать для интерпретации

6. Расписать разницу SHAP и Shaply values -- пока что не очень понятно

% привести к тому, что они показывают отклонение от среднего -- объясняют его -- это осталось из основного источника
% еще там прописан монте-карло опять