Метод SHAP несколько отличается от использования значений Шэпли напрямую для интерпретации вклада признаков -- он модифицирует классический подход. Рассмотрим SHAP подробнее.

Вместо использования большого количества моделей мы обучим одну модель и далее будем пользоваться только ее предсказаниями. Тогда наша модель представима в виде $f: \re^d \rightarrow \re$. Вместо исключения из нее признаков (в таком случае нам пришлось бы переобучать модель) мы оставим данные признаки в виде случайных величин и посчитаем математическое ожидание предсказания при фиксированных включенных в модель признаках.

Для упрощения расчетов мы не будем рассматривать разные значения предикторов, а бинаризуем их представление, обозначив за <<1>> наличие предиктора и за <<0>> его отсутствие: $x \rightarrow x', \, x' = \{1\}^d$. % не знаю корректно ли
Обозначим за $z' \in \{0,1\}^d$ объект, у которого мы учитываем только признаки из $S$ при формировании предсказания:
$z_j =
\begin{cases}
1, \text{если $j \in S$}\\
0, \text{если $j \notin S$}\\
\end{cases}$.\\
То есть $z'$ -- одна из возможных комбинаций предикторов.

Обученная модель работает с исходным видом признаков, поэтому нам нужно также восстанавливать значения исходных признаков по бинарному вектору. Введем для этого функцию $h_x(z') = z$, где $x$ -- исходный вектор исследуемого объекта, $z'$ -- бинарный вектор, в котором некоторые признаки заменены нулями, $z$ -- представление вектора $z'$ в исходном пространстве признаков. Функция $h_x$ вместо единиц восстанавливает значения из вектора $x$, а вместо нулей оставляет признак как некоторую переменную (случайную величину).
% наверняка можно как-то умнее упрощать признаки
Тогда наше ожидаемое предсказание для $z$ представимо в виде математического ожидания:
\[
\bar{f}(z) = \e(f(z)|\,z_S)
\]

Мы знаем конкретные значения признаков $z_S$, так как функция $h_x$ перенесла их из объекта $x$. Найдя математическое ожидание мы можем подставить данные значения, чтобы получить условное математическое ожидание, которое и будет оценкой нашего предсказания. И уже данную формулу мы можем использовать при расчете необходимых значений.% здесь для его расчета мы почему-то переходили к безусловному матожиданию, это связано со свойствами значений Шэпли -- не уверена нужно ли это прописывать

Поскольку мы не знаем истинное распределение, чтобы иметь возможность посчитать матожидание, аппроксимируем его оценкой. Для этого немного изменим функцию $h_x$ -- теперь вместо нулей она будет проставлять случайные значения соответствующих признаков. Тогда для каждого множества $S$ мы сможем получить несколько предсказаний модели и усреднить их:
\[
\hat{f}(z) = \frac{1}{k} \sum\limits_{i=1}^k f(z_{F\backslash S}, z_S),
\]
где $z_{F\backslash S}$ -- исключенные из модели признаки, вместо которые подставлены случайные значения, $z_S$ -- включенные в модель признаки, значения которых известны из исследуемого объекта $x$.
\paragraph{Kernel SHAP.}
Авторы алгоритма показали, что есть упрощенный способ найти значения Шэпли: % с оговоркой про то что это не значения Шэпли
с помощью взвешенного МНК. При матрице весов 
\[W = \frac{|F|-1}{
\begin{pmatrix}
|F| \\
|S|
\end{pmatrix} \cdot |S| \cdot (|F| - |S|)} = \frac{(|F|-1)}{|S|} \cdot \frac{1}{|F|} \cdot \frac{|S|! \cdot (|F|-|S|-1)!}{(|F|-1)!}
\]
коэффициенты при признаках в линейной регрессии являются соответствующими значениями Шэпли, которые можно использовать для интерпретации их вклада в предсказание.

Для данного способа аналогично перебираются разные комбинации $z'$ наличия/отсутствия признаков и переводятся в исходное пространство признаков $z = h_x(z')$ -- так формируется выборка $Z$ для регрессии. Зависимой переменной является предсказание модели для сгенерированной выборки: $y = f(h_x(z'))$. Тогда решением нашей задачи является вектор: \[
\phi = (Z^T W Z)^{-1} Z^T W y
\] % нужно ли говорить про корреляцию признаков

% константа -- когда нет признаков
% упрощение модели за счет снижения бюджета
% регуляризация

% не показала связь с линейным лаймом
% нужно будет конкретно показать, как интепретировать SHAP

% показать что графики похожи на PDP -- необязательно
% нужно ли указывать свойства? я могу и без них доказать
% расписать разные виды SHAP, точно указать Tree


% привести к тому, что они показывают отклонение от среднего -- объясняют его -- это осталось из основного источника -- не указала это -- это есть и в оригинальной статье -- нужно добавить