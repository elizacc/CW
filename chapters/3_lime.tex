LIME (Local Interpretable Model-Agnostic Explanations) -- метод, показывающий вклад признаков в отдельное предсказание, работающий с любой моделью.
% нужен ли перевод
% добавить пример работы

\subsubsection{Идея}
Результаты некоторых моделей легко интерпретировать. Например, в линейной регрессии можно посмотреть на веса. Они показывают, насколько изменится предсказание при изменении признаков. Так для каждого конкретного предсказания можно понять, почему модель выдала именно такой результат -- виден непосредственный вклад каждого признака.

Но не все модели легко интерпретировать. Например, некоторые архитектуры нейронных сетей. Они зачастую значительно превосходят линейные модели, но при этом сама структура модели представляет собой <<черный ящик>> -- непонятно, как именно модель сформировала предсказание, какие признаки сильнее повлияли на решение нейронной сети.

Идея состоит в том, чтобы перенести свойство интерпретируемости простых моделей на более сложные. Мы можем обучить интерпретируемую модель по выборке, где ответами являются предсказания сложной модели. В процессе обучения модель анализирует зависимости непосредственно между признаками и предсказаниями сложной модели. Тогда мы сможем интерпретировать результаты простой модели, которые являются аппроксимацией предсказаний сложной модели.

Возникает проблема: сложная модель выявляет зависимости, которые, например, линейная модель может не уловить. Но мы можем воспользоваться свойством, что дифференцируемые функции можно линеаризовать в окрестности заданной точки. То есть, если мы будем рассматривать одно предсказание, то в его небольшой окрестности мы можем считать простую модель аппроксимацией более сложной.

% почему для любой модели - это можно будет в каких-нибудь выводах написать
% почему не вытаскивать из самих моделей?
% добавить картинку про локалити

<Свой пример для чиселок>